<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Professional Face Blur Tool</title>
  <style>
    /* Neon UI Styling */
    :root {
      --neon-blue: #0ff;
      --background: #000;
    }
    body {
      background: var(--background);
      color: #fff;
      font-family: Arial;
      text-align: center;
    }
    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }
    video, canvas {
      width: 100%;
      border: 3px solid var(--neon-blue);
      border-radius: 10px;
      box-shadow: 0 0 15px var(--neon-blue);
    }
    .controls {
      margin: 20px;
    }
    button {
      background: var(--neon-blue);
      color: #000;
      padding: 12px 25px;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      margin: 10px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üî∑ Professional Face Blur</h1>
    <input type="file" id="videoInput" accept="video/*" hidden>
    <div class="controls">
      <button onclick="document.getElementById('videoInput').click()">üìÅ Upload Video</button>
      <button id="processBtn" disabled>‚ö° Process Video</button>
      <button id="downloadBtn" disabled>üíæ Download</button>
    </div>
    <video id="inputVideo" controls></video>
    <canvas id="outputCanvas"></canvas>
  </div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.18.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface@0.0.7/dist/blazeface.min.js"></script>
<script>
const videoInput = document.getElementById('videoInput');
const inputVideo = document.getElementById('inputVideo');
const outputCanvas = document.getElementById('outputCanvas');
const processBtn = document.getElementById('processBtn');
const downloadBtn = document.getElementById('downloadBtn');

let model;
let ctx;
let mediaRecorder;
let chunks = [];

// 1. Initialize WebGL context
const initWebGL = () => {
  const gl = outputCanvas.getContext('webgl');
  if (!gl) {
    console.error('WebGL not supported');
    return null;
  }
  return gl;
};

// 2. High-quality Gaussian Blur Shader
const blurShader = `
precision highp float;
varying vec2 vTexCoord;
uniform sampler2D uImage;
uniform vec2 uDirection;
uniform float uSigma;

float gaussian(float x, float sigma) {
  return exp(-(x * x) / (2.0 * sigma * sigma)) / (sqrt(2.0 * 3.14159265) * sigma);
}

void main() {
  vec4 color = vec4(0.0);
  float total = 0.0;
  int kernelSize = int(ceil(uSigma * 3.0)) * 2 + 1;
  
  for (int i = -kernelSize; i <= kernelSize; i++) {
    float weight = gaussian(float(i), uSigma);
    color += texture2D(uImage, vTexCoord + uDirection * float(i)) * weight;
    total += weight;
  }
  
  gl_FragColor = color / total;
}`;

// 3. WebGL Blur Pipeline
class BlurProcessor {
  constructor(gl) {
    this.gl = gl;
    this.program = this.createProgram(blurShader);
    this.framebuffer = gl.createFramebuffer();
  }

  createProgram(shaderSource) {
    const gl = this.gl;
    const vertexShader = gl.createShader(gl.VERTEX_SHADER);
    gl.shaderSource(vertexShader, 'attribute vec2 aPosition; varying vec2 vTexCoord; void main() { gl_Position = vec4(aPosition, 0, 1); vTexCoord = aPosition*0.5+0.5; }');
    gl.compileShader(vertexShader);

    const fragmentShader = gl.createShader(gl.FRAGMENT_SHADER);
    gl.shaderSource(fragmentShader, shaderSource);
    gl.compileShader(fragmentShader);

    const program = gl.createProgram();
    gl.attachShader(program, vertexShader);
    gl.attachShader(program, fragmentShader);
    gl.linkProgram(program);
    return program;
  }

  applyBlur(texture, width, height, sigma) {
    const gl = this.gl;
    
    // Horizontal pass
    gl.bindFramebuffer(gl.FRAMEBUFFER, this.framebuffer);
    gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, texture, 0);
    
    gl.useProgram(this.program);
    const directionLoc = gl.getUniformLocation(this.program, 'uDirection');
    const sigmaLoc = gl.getUniformLocation(this.program, 'uSigma');
    
    gl.uniform2f(directionLoc, 1/width, 0);
    gl.uniform1f(sigmaLoc, sigma);
    gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);

    // Vertical pass
    gl.bindFramebuffer(gl.FRAMEBUFFER, null);
    gl.uniform2f(directionLoc, 0, 1/height);
    gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);

    return texture;
  }
}

// 4. Main Processing Logic
async function processVideo() {
  const gl = initWebGL();
  if (!gl) return;

  const blurProcessor = new BlurProcessor(gl);
  const stream = outputCanvas.captureStream(30);
  mediaRecorder = new MediaRecorder(stream, { 
    mimeType: 'video/webm; codecs=vp9',
    videoBitsPerSecond: 25000000 // 25 Mbps
  });

  mediaRecorder.ondataavailable = e => chunks.push(e.data);
  mediaRecorder.start();

  let lastTime = performance.now();
  
  async function processFrame() {
    const now = performance.now();
    const delta = now - lastTime;
    lastTime = now;

    // Face detection
    const predictions = await model.estimateFaces(inputVideo, false);
    
    // WebGL processing
    gl.viewport(0, 0, outputCanvas.width, outputCanvas.height);
    const texture = gl.createTexture();
    gl.bindTexture(gl.TEXTURE_2D, texture);
    gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, inputVideo);

    // Apply blur to detected faces
    predictions.forEach(face => {
      const [x, y, w, h] = [
        face.topLeft[0], 
        face.topLeft[1],
        face.bottomRight[0] - face.topLeft[0],
        face.bottomRight[1] - face.topLeft[1]
      ];
      
      // Adaptive blur parameters
      const faceArea = w * h;
      const sigma = Math.min(50, Math.max(10, faceArea / 1000));
      const padding = sigma * 0.5;

      // Set viewport to face region
      gl.viewport(
        x - padding, 
        outputCanvas.height - y - h - padding, 
        w + padding*2, 
        h + padding*2
      );

      // Apply Gaussian blur
      blurProcessor.applyBlur(texture, w + padding*2, h + padding*2, sigma);
    });

    requestAnimationFrame(processFrame);
  }

  inputVideo.addEventListener('play', () => {
    outputCanvas.width = inputVideo.videoWidth;
    outputCanvas.height = inputVideo.videoHeight;
    processFrame();
  });
}

// 5. Initialization
videoInput.addEventListener('change', async e => {
  const file = e.target.files[0];
  if (!file) return;

  inputVideo.src = URL.createObjectURL(file);
  inputVideo.onloadedmetadata = () => {
    processBtn.disabled = false;
  };
});

processBtn.addEventListener('click', async () => {
  model = await blazeface.load();
  processVideo();
});

downloadBtn.addEventListener('click', () => {
  mediaRecorder.stop();
  mediaRecorder.onstop = () => {
    const blob = new Blob(chunks, { type: 'video/webm' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'blurred-video.webm';
    a.click();
  };
});
</script>
</body>
</html>