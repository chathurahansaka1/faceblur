<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Responsive High Quality Face Blur</title>
  <!-- Load TensorFlow.js and the BlazeFace model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
  <style>
    :root {
      --primary-color: #00ff87;
      --background: #1e1e1e;
      --highlight-color: #00cc70;
    }
    body {
      margin: 0;
      padding: 20px;
      background: var(--background);
      color: #fff;
      font-family: sans-serif;
      text-align: center;
    }
    h1 {
      color: var(--primary-color);
      margin-bottom: 20px;
    }
    .controls {
      margin: 20px;
    }
    button, .custom-file-upload {
      background: var(--primary-color);
      color: #000;
      border: none;
      padding: 10px 20px;
      cursor: pointer;
      border-radius: 5px;
      font-size: 16px;
      margin: 10px;
      transition: transform 0.3s, background 0.3s;
    }
    button:hover, .custom-file-upload:hover {
      transform: scale(1.05);
      background: var(--highlight-color);
    }
    canvas {
      border: 2px solid var(--primary-color);
      border-radius: 5px;
      width: 100%;
      max-width: 800px;
      height: auto;
      margin: 20px auto;
      display: block;
    }
  </style>
</head>
<body>
  <h1>Responsive High Quality Face Blur</h1>
  <div class="controls">
    <label class="custom-file-upload">
      Upload Video
      <input type="file" id="videoInput" accept="video/*" style="display: none;">
    </label>
    <button id="downloadBtn" disabled>Download Blurred Video</button>
  </div>
  <canvas id="outputCanvas"></canvas>
  
  <script>
    const videoInput = document.getElementById('videoInput');
    const downloadBtn = document.getElementById('downloadBtn');
    const canvas = document.getElementById('outputCanvas');
    const ctx = canvas.getContext('2d');
    
    let videoElement;
    let model;
    let mediaRecorder;
    let recordedChunks = [];
    let processing = false;
    let frameCount = 0;
    let predictionsCache = [];
    
    // Load the BlazeFace model asynchronously.
    async function loadModel() {
      model = await blazeface.load();
      console.log("Model loaded.");
    }
    loadModel();
    
    videoInput.addEventListener('change', async () => {
      const file = videoInput.files[0];
      if (!file) return;
      
      recordedChunks = []; // Clear previous recordings
      
      // Create a hidden video element for processing.
      videoElement = document.createElement('video');
      videoElement.src = URL.createObjectURL(file);
      videoElement.playsInline = true;
      videoElement.muted = false; // Audio will be captured.
      videoElement.style.display = 'none';
      
      // Wait for metadata.
      await new Promise(resolve => videoElement.onloadedmetadata = resolve);
      
      // Set canvas intrinsic resolution to match the video.
      canvas.width = videoElement.videoWidth;
      canvas.height = videoElement.videoHeight;
      ctx.imageSmoothingEnabled = true;
      ctx.imageSmoothingQuality = 'high';
      
      // Determine frame rate (default to 30 if not available).
      let fps = 30;
      const videoTracks = videoElement.captureStream().getVideoTracks();
      if (videoTracks.length > 0) {
        const settings = videoTracks[0].getSettings();
        if (settings.frameRate) fps = settings.frameRate;
      }
      
      // Capture the canvas stream at the videoâ€™s frame rate.
      const canvasStream = canvas.captureStream(fps);
      // Add the original audio track.
      const audioTracks = videoElement.captureStream().getAudioTracks();
      if (audioTracks.length > 0) {
        canvasStream.addTrack(audioTracks[0]);
      }
      
      // Calculate a high bitrate (multiplier of 30) to preserve quality.
      const bitRate = videoElement.videoWidth * videoElement.videoHeight * 30;
      
      try {
        mediaRecorder = new MediaRecorder(canvasStream, {
          mimeType: 'video/webm;codecs=vp9',
          videoBitsPerSecond: bitRate
        });
      } catch (e) {
        alert("MediaRecorder initialization failed: " + e);
        return;
      }
      
      mediaRecorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          recordedChunks.push(e.data);
        }
      };
      mediaRecorder.onstop = () => {
        downloadBtn.disabled = false;
      };
      mediaRecorder.start();
      
      processing = true;
      frameCount = 0;
      
      // Start video playback.
      videoElement.play();
      
      // Use requestVideoFrameCallback if available for precise sync.
      if (videoElement.requestVideoFrameCallback) {
        videoElement.requestVideoFrameCallback(processFrame);
      } else {
        requestAnimationFrame(processFrameFallback);
      }
    });
    
    // Process frames using requestVideoFrameCallback.
    async function processFrame(now, metadata) {
      if (!processing) return;
      frameCount++;
      
      // Draw the current video frame.
      ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
      
      // Update face detection every 5 frames.
      if (frameCount % 5 === 0) {
        try {
          predictionsCache = await model.estimateFaces(videoElement);
        } catch (err) {
          console.error("Face detection error:", err);
        }
      }
      
      // Apply a 20px blur to each detected face region.
      predictionsCache.forEach(pred => {
        const [x, y] = pred.topLeft;
        const [x2, y2] = pred.bottomRight;
        const width = x2 - x;
        const height = y2 - y;
        ctx.save();
        ctx.beginPath();
        ctx.rect(x, y, width, height);
        ctx.clip();
        ctx.filter = 'blur(20px)';
        ctx.drawImage(canvas, x, y, width, height, x, y, width, height);
        ctx.restore();
      });
      
      // Continue processing if the video is still playing.
      if (!videoElement.paused && !videoElement.ended) {
        videoElement.requestVideoFrameCallback(processFrame);
      } else {
        processing = false;
        mediaRecorder.stop();
      }
    }
    
    // Fallback using requestAnimationFrame.
    async function processFrameFallback() {
      if (!processing) return;
      frameCount++;
      ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
      
      if (frameCount % 5 === 0) {
        try {
          predictionsCache = await model.estimateFaces(videoElement);
        } catch (err) {
          console.error("Face detection error:", err);
        }
      }
      
      predictionsCache.forEach(pred => {
        const [x, y] = pred.topLeft;
        const [x2, y2] = pred.bottomRight;
        const width = x2 - x;
        const height = y2 - y;
        ctx.save();
        ctx.beginPath();
        ctx.rect(x, y, width, height);
        ctx.clip();
        ctx.filter = 'blur(20px)';
        ctx.drawImage(canvas, x, y, width, height, x, y, width, height);
        ctx.restore();
      });
      
      if (!videoElement.paused && !videoElement.ended) {
        requestAnimationFrame(processFrameFallback);
      } else {
        processing = false;
        mediaRecorder.stop();
      }
    }
    
    downloadBtn.addEventListener('click', () => {
      const blob = new Blob(recordedChunks, { type: 'video/webm' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = `blurred-video-${Date.now()}.webm`;
      document.body.appendChild(a);
      a.click();
      URL.revokeObjectURL(url);
    });
  </script>
</body>
</html>