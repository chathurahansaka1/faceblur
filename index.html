<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Face Blur Processing with Audio & Enhanced Quality</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
  <style>
    :root {
      --primary-color: #00ff87;
      --background: #2d2d2d;
    }
    body {
      margin: 0;
      padding: 10px;
      background: var(--background);
      color: #fff;
      font-family: 'Segoe UI', sans-serif;
      text-align: center;
    }
    .controls {
      margin: 15px 0;
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      justify-content: center;
    }
    .custom-file-upload {
      display: inline-block;
      cursor: pointer;
      background: var(--primary-color);
      color: #000;
      padding: 10px 20px;
      border-radius: 8px;
      font-size: 14px;
      transition: transform 0.2s, background 0.3s;
    }
    .custom-file-upload:hover {
      background: #00cc70;
      transform: scale(1.05);
    }
    .slider-container {
      background: rgba(255, 255, 255, 0.1);
      padding: 10px;
      border-radius: 8px;
      width: 100%;
      max-width: 300px;
    }
    a.download-btn {
      background: var(--primary-color);
      color: #000;
      border: none;
      padding: 10px 20px;
      border-radius: 8px;
      font-size: 14px;
      transition: transform 0.2s, background 0.3s;
      text-decoration: none;
      pointer-events: none;
    }
    a.download-btn:hover {
      background: #00cc70;
      transform: scale(1.05);
    }
    .loader {
      display: none;
      border: 4px solid #f3f3f3;
      border-top: 4px solid var(--primary-color);
      border-radius: 50%;
      width: 30px;
      height: 30px;
      animation: spin 1s linear infinite;
      margin: 10px auto;
    }
    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
    /* Hide the video element from view */
    .hidden-video {
      display: none;
    }
    /* Position the canvas off-screen while still rendering at full resolution */
    .hidden-canvas {
      position: absolute;
      top: -9999px;
      left: -9999px;
    }
  </style>
</head>
<body>
  <h1>Face Blur Processing</h1>
  <div class="controls">
    <label for="videoInput" class="custom-file-upload">Choose Video</label>
    <input type="file" id="videoInput" accept="video/*" style="display:none;">
    <div class="slider-container">
      <label for="blurStrength">Blur Strength: <span id="blurValue">20</span>px</label>
      <input type="range" id="blurStrength" min="5" max="50" value="20">
    </div>
    <a id="downloadBtn" class="download-btn" download>Download Processed Video</a>
  </div>
  <div class="loader" id="loader"></div>
  
  <!-- Hidden elements for off-screen processing -->
  <video id="inputVideo" class="hidden-video" playsinline></video>
  <canvas id="outputCanvas" class="hidden-canvas"></canvas>

  <script>
    const videoInput = document.getElementById('videoInput');
    const inputVideo = document.getElementById('inputVideo');
    const outputCanvas = document.getElementById('outputCanvas');
    const downloadBtn = document.getElementById('downloadBtn');
    const blurStrength = document.getElementById('blurStrength');
    const blurValue = document.getElementById('blurValue');
    const loader = document.getElementById('loader');

    let model;
    let mediaRecorder;
    let recordedChunks = [];
    let blurLevel = 20;
    let animationFrameId;
    let frameTimes = [];

    // Load the Blazeface model
    async function loadModel() {
      loader.style.display = 'block';
      try {
        model = await blazeface.load();
        console.log("Model loaded!");
      } catch (error) {
        console.error("Model load failed:", error);
        alert("Error loading model. Check console.");
      }
      loader.style.display = 'none';
    }
    loadModel();

    // Update blur level from slider
    blurStrength.addEventListener('input', () => {
      blurLevel = blurStrength.value;
      blurValue.textContent = blurLevel;
    });

    // When a video file is chosen, process it automatically
    videoInput.addEventListener('change', async (e) => {
      const file = e.target.files[0];
      if (file) {
        recordedChunks = [];
        downloadBtn.style.pointerEvents = 'none';
        
        const url = URL.createObjectURL(file);
        inputVideo.src = url;
        await new Promise(resolve => inputVideo.onloadedmetadata = resolve);
        
        // Set canvas dimensions equal to the original video's resolution
        outputCanvas.width = inputVideo.videoWidth;
        outputCanvas.height = inputVideo.videoHeight;
        
        // Start video playback and processing
        inputVideo.currentTime = 0;
        inputVideo.play();
        startRecording();
        processVideo();
        
        // When the video ends, stop processing and finalize the recording
        inputVideo.onended = () => {
          stopProcessing();
        };
      }
    });

    // Process video frames off-screen
    async function processVideo() {
      if (inputVideo.paused || inputVideo.ended) return;
      
      const ctx = outputCanvas.getContext('2d');
      const startTime = performance.now();
      
      // Draw the current video frame at full resolution
      ctx.drawImage(inputVideo, 0, 0, outputCanvas.width, outputCanvas.height);
      
      try {
        // Detect faces in the current frame
        const predictions = await model.estimateFaces(inputVideo);
        if (predictions.length > 0) {
          predictions.forEach(pred => {
            const [x, y] = pred.topLeft;
            const [x2, y2] = pred.bottomRight;
            const w = x2 - x;
            const h = y2 - y;
            ctx.save();
            ctx.beginPath();
            ctx.rect(x, y, w, h);
            ctx.clip();
            ctx.filter = `blur(${blurLevel}px)`;
            // Redraw the clipped (face) region with blur
            ctx.drawImage(outputCanvas, x, y, w, h, x, y, w, h);
            ctx.restore();
          });
        }
        
        // Calculate processing time for this frame
        const duration = performance.now() - startTime;
        frameTimes.push(duration);
        if (frameTimes.length > 10) frameTimes.shift();
        const avgTime = frameTimes.reduce((a, b) => a + b, 0) / frameTimes.length;
        
        // Overlay processing stats (face count and processing time) on the frame
        ctx.save();
        ctx.font = "20px Arial";
        ctx.fillStyle = "white";
        ctx.fillText(`Faces: ${predictions.length}`, 10, 30);
        ctx.fillText(`Proc Time: ${avgTime.toFixed(1)}ms`, 10, 60);
        ctx.restore();
      } catch (error) {
        console.error("Processing error:", error);
      }
      
      animationFrameId = requestAnimationFrame(processVideo);
    }

    // Start recording the processed video along with audio from the original video
    function startRecording() {
      // Capture processed video from the canvas
      const canvasStream = outputCanvas.captureStream(30);
      
      // Use Web Audio API to capture audio from the video element reliably
      let audioTracks = [];
      try {
        const audioContext = new AudioContext();
        const sourceNode = audioContext.createMediaElementSource(inputVideo);
        const destinationNode = audioContext.createMediaStreamDestination();
        sourceNode.connect(destinationNode);
        audioTracks = destinationNode.stream.getAudioTracks();
      } catch (e) {
        console.warn("Audio capture via Web Audio API failed, falling back to captureStream", e);
        const videoStream = inputVideo.captureStream();
        audioTracks = videoStream.getAudioTracks();
      }
      
      // Combine the canvas video track and the captured audio tracks
      const combinedStream = new MediaStream([
        ...canvasStream.getVideoTracks(),
        ...audioTracks
      ]);
      
      // Increase bitrate for improved quality
      mediaRecorder = new MediaRecorder(combinedStream, {
        mimeType: 'video/webm;codecs=vp9',
        videoBitsPerSecond: 5e6
      });
      
      mediaRecorder.ondataavailable = e => {
        if (e.data && e.data.size > 0) {
          recordedChunks.push(e.data);
        }
      };
      mediaRecorder.onstop = exportVideo;
      mediaRecorder.start(100);
    }

    // Stop processing and recording when video ends
    function stopProcessing() {
      cancelAnimationFrame(animationFrameId);
      if (mediaRecorder && mediaRecorder.state !== "inactive") {
        mediaRecorder.stop();
      }
      inputVideo.pause();
    }

    // Combine recorded chunks, create a Blob, and enable the download button
    async function exportVideo() {
      loader.style.display = 'block';
      const blob = new Blob(recordedChunks, { type: 'video/webm' });
      const url = URL.createObjectURL(blob);
      downloadBtn.href = url;
      downloadBtn.download = `processed-video-${Date.now()}.webm`;
      downloadBtn.style.pointerEvents = 'auto';
      loader.style.display = 'none';
    }
  </script>
</body>
</html>